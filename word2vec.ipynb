{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import collections\n",
    "import numpy as np\n",
    "from keras.layers import Input, Embedding, Reshape, Lambda, Dot, Dense\n",
    "from keras import Model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ HELPER FUNCTIONS ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, filename):\n",
    "    f = open(path+filename, \"r\")\n",
    "    data = tf.compat.as_str(f.read()).split()\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ PREPARE DATASET ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "window_size = 3\n",
    "vector_dim = 300\n",
    "epochs = 1000000\n",
    "valid_size = 1\n",
    "validation_words = ['eight']\n",
    "\n",
    "vocabulary = read_data(\"dataset/\", \"text8.txt\")\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary, vocab_size)\n",
    "# valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "valid_examples = [dictionary[w] for w in validation_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ MAKE SAMPLING TABLE ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ MAKE WORD2VEC MODEL ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "dot_product = Dot(axes=(1,1))([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ SIMILARITY CALLBACK ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_words(test_word):\n",
    "    sim = dict()\n",
    "    emb_1 = np.array(embedding(dictionary[test_word]))\n",
    "    for w in dictionary:\n",
    "        emb_2 = np.array(embedding(dictionary[w]))\n",
    "        sim[w] = np.dot(emb_1, emb_2) / np.linalg.norm(emb_1) / np.linalg.norm(emb_2)\n",
    "    mydict = {k: v for k, v in sorted(sim.items(), key=lambda item: item[1])}\n",
    "    return {k: mydict[k] for k in list(mydict)[-10:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ TRAINING&VALIDATION ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6000, loss=0.861236035823822\n",
      "Iteration 156100, loss=0.7527545690536499\n",
      "Iteration 156200, loss=0.3800755739212036\n",
      "Iteration 156300, loss=0.5544195771217346\n",
      "Iteration 156400, loss=0.5299660563468933\n",
      "Iteration 156500, loss=0.05654020980000496\n",
      "Iteration 156600, loss=0.3988248407840729\n",
      "Iteration 156700, loss=0.5295549035072327\n",
      "Iteration 156800, loss=3.1463162741829365e-08\n",
      "Iteration 156900, loss=0.6760252714157104\n",
      "Iteration 157000, loss=1.1998937129974365\n",
      "Iteration 157100, loss=1.2775481939315796\n",
      "Iteration 157200, loss=0.4804970622062683\n",
      "Iteration 157300, loss=0.5288588404655457\n",
      "Iteration 157400, loss=0.6456080079078674\n",
      "Iteration 157500, loss=0.5693507194519043\n",
      "Iteration 157600, loss=0.3205789029598236\n",
      "Iteration 157700, loss=0.7414059638977051\n",
      "Iteration 157800, loss=0.5153306722640991\n",
      "Iteration 157900, loss=0.8547376394271851\n",
      "Iteration 158000, loss=0.20423564314842224\n",
      "Iteration 158100, loss=0.5103796124458313\n",
      "Iteration 158200, loss=0.3407629430294037\n",
      "Iteration 158300, loss=1.134137749671936\n",
      "Iteration 158400, loss=0.8424345254898071\n",
      "Iteration 158500, loss=0.9002280235290527\n",
      "Iteration 158600, loss=0.685722291469574\n",
      "Iteration 158700, loss=0.2585730254650116\n",
      "Iteration 158800, loss=0.5140709280967712\n",
      "Iteration 158900, loss=0.8377490043640137\n",
      "Iteration 159000, loss=0.5588594079017639\n",
      "Iteration 159100, loss=0.8054594397544861\n",
      "Iteration 159200, loss=0.09097175300121307\n",
      "Iteration 159300, loss=0.06810479611158371\n",
      "Iteration 159400, loss=0.5637037754058838\n",
      "Iteration 159500, loss=0.05898530036211014\n",
      "Iteration 159600, loss=0.5232065320014954\n",
      "Iteration 159700, loss=0.5058395862579346\n",
      "Iteration 159800, loss=0.5252062082290649\n",
      "Iteration 159900, loss=0.3089044988155365\n",
      "Iteration 160000, loss=0.22229218482971191\n",
      "{'mid': 0.82263935, 'm': 0.82298726, 'admiral': 0.82408494, 'zero': 0.830351, 'j': 0.83233166, 'microsoft': 0.83394367, 'defined': 0.8390898, 'circa': 0.8470181, 'one': 0.8560752, 'eight': 1.0000001}\n",
      "Iteration 160100, loss=0.7470954656600952\n",
      "Iteration 160200, loss=0.5222081542015076\n",
      "Iteration 160300, loss=0.6932573318481445\n",
      "Iteration 160400, loss=0.7369948625564575\n",
      "Iteration 160500, loss=0.5346674919128418\n",
      "Iteration 160600, loss=0.5342743992805481\n",
      "Iteration 160700, loss=0.5468173623085022\n",
      "Iteration 160800, loss=0.4508596956729889\n",
      "Iteration 160900, loss=0.4361889064311981\n",
      "Iteration 161000, loss=1.215830683708191\n",
      "Iteration 161100, loss=0.5055961012840271\n",
      "Iteration 161200, loss=0.3247227966785431\n",
      "Iteration 161300, loss=0.45981162786483765\n",
      "Iteration 161400, loss=0.924484372138977\n",
      "Iteration 161500, loss=0.6052061915397644\n",
      "Iteration 161600, loss=0.5313441753387451\n",
      "Iteration 161700, loss=0.5286537408828735\n",
      "Iteration 161800, loss=0.4696424603462219\n",
      "Iteration 161900, loss=0.49941253662109375\n",
      "Iteration 162000, loss=0.5285844802856445\n",
      "Iteration 162100, loss=0.5591919422149658\n",
      "Iteration 162200, loss=0.45397740602493286\n",
      "Iteration 162300, loss=0.21008074283599854\n",
      "Iteration 162400, loss=0.5033307671546936\n",
      "Iteration 162500, loss=0.9187108278274536\n",
      "Iteration 162600, loss=0.5201651453971863\n",
      "Iteration 162700, loss=0.9604052901268005\n",
      "Iteration 162800, loss=0.5049137473106384\n",
      "Iteration 162900, loss=0.6343174576759338\n",
      "Iteration 163000, loss=0.8725101947784424\n",
      "Iteration 163100, loss=0.49990659952163696\n",
      "Iteration 163200, loss=0.5091685652732849\n",
      "Iteration 163300, loss=0.350555956363678\n",
      "Iteration 163400, loss=0.4723980724811554\n",
      "Iteration 163500, loss=0.7956340312957764\n",
      "Iteration 163600, loss=0.5975468158721924\n",
      "Iteration 163700, loss=0.6148994565010071\n",
      "Iteration 163800, loss=0.5162325501441956\n",
      "Iteration 163900, loss=0.5413882732391357\n",
      "Iteration 164000, loss=0.49168065190315247\n",
      "Iteration 164100, loss=0.07149277627468109\n",
      "Iteration 164200, loss=0.5558705925941467\n",
      "Iteration 164300, loss=0.3210422992706299\n",
      "Iteration 164400, loss=0.8388085961341858\n",
      "Iteration 164500, loss=0.5141443014144897\n",
      "Iteration 164600, loss=0.5215967893600464\n",
      "Iteration 164700, loss=0.5569049715995789\n",
      "Iteration 164800, loss=0.6099629402160645\n",
      "Iteration 164900, loss=0.3864613473415375\n",
      "Iteration 165000, loss=0.009145117364823818\n",
      "Iteration 165100, loss=0.29335781931877136\n",
      "Iteration 165200, loss=0.5850468873977661\n",
      "Iteration 165300, loss=0.4642707109451294\n",
      "Iteration 165400, loss=1.0234627723693848\n",
      "Iteration 165500, loss=1.1958177089691162\n",
      "Iteration 165600, loss=0.5257046222686768\n",
      "Iteration 165700, loss=1.114747166633606\n",
      "Iteration 165800, loss=0.7052119374275208\n",
      "Iteration 165900, loss=0.6407417058944702\n",
      "Iteration 166000, loss=0.906366229057312\n",
      "Iteration 166100, loss=0.04078211262822151\n",
      "Iteration 166200, loss=0.00236899103038013\n",
      "Iteration 166300, loss=0.4564180076122284\n",
      "Iteration 166400, loss=0.5242676138877869\n",
      "Iteration 166500, loss=0.9141110181808472\n",
      "Iteration 166600, loss=0.8222104907035828\n",
      "Iteration 166700, loss=0.5253122448921204\n",
      "Iteration 166800, loss=9.261545528715942e-06\n",
      "Iteration 166900, loss=0.4909696877002716\n",
      "Iteration 167000, loss=0.5433125495910645\n",
      "Iteration 167100, loss=0.879547119140625\n",
      "Iteration 167200, loss=0.524841845035553\n",
      "Iteration 167300, loss=0.8891069293022156\n",
      "Iteration 167400, loss=0.49182119965553284\n",
      "Iteration 167500, loss=0.4065147340297699\n",
      "Iteration 167600, loss=0.4672393500804901\n",
      "Iteration 167700, loss=0.04615361616015434\n",
      "Iteration 167800, loss=0.487591415643692\n",
      "Iteration 167900, loss=0.5463401675224304\n",
      "Iteration 168000, loss=0.9094558954238892\n",
      "Iteration 168100, loss=0.7115877866744995\n",
      "Iteration 168200, loss=0.9396399855613708\n",
      "Iteration 168300, loss=0.8652606010437012\n",
      "Iteration 168400, loss=0.6538587212562561\n",
      "Iteration 168500, loss=0.6491488218307495\n",
      "Iteration 168600, loss=0.5905434489250183\n",
      "Iteration 168700, loss=0.4130995571613312\n",
      "Iteration 168800, loss=0.5513227581977844\n",
      "Iteration 168900, loss=0.8722834587097168\n",
      "Iteration 169000, loss=0.6368644833564758\n",
      "Iteration 169100, loss=0.5547053217887878\n",
      "Iteration 169200, loss=0.9776852130889893\n",
      "Iteration 169300, loss=0.5286849737167358\n",
      "Iteration 169400, loss=0.07238586992025375\n",
      "Iteration 169500, loss=0.5925995111465454\n",
      "Iteration 169600, loss=0.5588392615318298\n",
      "Iteration 169700, loss=0.6638954281806946\n",
      "Iteration 169800, loss=0.057571832090616226\n",
      "Iteration 169900, loss=1.1876615285873413\n",
      "Iteration 170000, loss=0.23775915801525116\n",
      "{'drake': 0.8339033, 'zero': 0.8343891, 'three': 0.8344877, 'microsoft': 0.83602697, 'm': 0.8368086, 's': 0.84016335, 'circa': 0.84501386, 'j': 0.84712946, 'one': 0.86071, 'eight': 1.0}\n",
      "Iteration 170100, loss=0.8621770143508911\n",
      "Iteration 170200, loss=0.6041191220283508\n",
      "Iteration 170300, loss=0.720471978187561\n",
      "Iteration 170400, loss=0.7229479551315308\n",
      "Iteration 170500, loss=0.0796147957444191\n",
      "Iteration 170600, loss=0.6406865119934082\n",
      "Iteration 170700, loss=0.7645405530929565\n",
      "Iteration 170800, loss=0.11279941350221634\n",
      "Iteration 170900, loss=0.5216830372810364\n",
      "Iteration 171000, loss=0.566364049911499\n",
      "Iteration 171100, loss=0.8754938244819641\n",
      "Iteration 171200, loss=0.5788479447364807\n",
      "Iteration 171300, loss=0.579353928565979\n",
      "Iteration 171400, loss=0.3321785032749176\n",
      "Iteration 171500, loss=0.38541555404663086\n",
      "Iteration 171600, loss=0.6672228574752808\n",
      "Iteration 171700, loss=0.6336875557899475\n",
      "Iteration 171800, loss=0.28076374530792236\n",
      "Iteration 171900, loss=1.0149056911468506\n",
      "Iteration 172000, loss=0.3119811713695526\n",
      "Iteration 172100, loss=0.6542680263519287\n",
      "Iteration 172200, loss=0.5264927744865417\n",
      "Iteration 172300, loss=0.511726975440979\n",
      "Iteration 172400, loss=0.34001368284225464\n",
      "Iteration 172500, loss=0.2781311869621277\n",
      "Iteration 172600, loss=0.5213444232940674\n",
      "Iteration 172700, loss=0.48169830441474915\n",
      "Iteration 172800, loss=0.5342644453048706\n",
      "Iteration 172900, loss=0.02676316909492016\n",
      "Iteration 173000, loss=0.475576788187027\n",
      "Iteration 173100, loss=0.9720871448516846\n",
      "Iteration 173200, loss=0.9385778903961182\n",
      "Iteration 173300, loss=0.3981160819530487\n",
      "Iteration 173400, loss=0.017630113288760185\n",
      "Iteration 173500, loss=0.5274757146835327\n",
      "Iteration 173600, loss=0.5336836576461792\n",
      "Iteration 173700, loss=0.5371779203414917\n",
      "Iteration 173800, loss=0.5336740612983704\n",
      "Iteration 173900, loss=0.5343364477157593\n",
      "Iteration 174000, loss=0.6646128296852112\n",
      "Iteration 174100, loss=1.121633529663086\n",
      "Iteration 174200, loss=0.9996657371520996\n",
      "Iteration 174300, loss=0.5524982213973999\n",
      "Iteration 174400, loss=0.046306952834129333\n",
      "Iteration 174500, loss=0.9254497289657593\n",
      "Iteration 174600, loss=0.6745519042015076\n",
      "Iteration 174700, loss=0.5007946491241455\n",
      "Iteration 174800, loss=0.6810594797134399\n",
      "Iteration 174900, loss=0.4637623727321625\n",
      "Iteration 175000, loss=0.8200227618217468\n",
      "Iteration 175100, loss=0.49141615629196167\n",
      "Iteration 175200, loss=2.9171741008758545\n",
      "Iteration 175300, loss=0.3666031062602997\n",
      "Iteration 175400, loss=0.3175854980945587\n",
      "Iteration 175500, loss=0.12993164360523224\n",
      "Iteration 175600, loss=0.6394716501235962\n",
      "Iteration 175700, loss=0.0007731093792244792\n",
      "Iteration 175800, loss=0.5216371417045593\n",
      "Iteration 175900, loss=0.5099404454231262\n",
      "Iteration 176000, loss=0.5282561182975769\n",
      "Iteration 176100, loss=1.0946907997131348\n",
      "Iteration 176200, loss=0.6311392188072205\n",
      "Iteration 176300, loss=0.4443136155605316\n",
      "Iteration 176400, loss=0.9025801420211792\n",
      "Iteration 176500, loss=1.0209579467773438\n",
      "Iteration 176600, loss=0.5155959129333496\n",
      "Iteration 176700, loss=0.897568941116333\n",
      "Iteration 176800, loss=0.5214579701423645\n",
      "Iteration 176900, loss=0.1553545892238617\n",
      "Iteration 177000, loss=0.7022071480751038\n",
      "Iteration 177100, loss=0.25968465209007263\n",
      "Iteration 177200, loss=0.06111227348446846\n",
      "Iteration 177300, loss=0.9400384426116943\n",
      "Iteration 177400, loss=0.6377968192100525\n",
      "Iteration 177500, loss=0.6540760397911072\n",
      "Iteration 177600, loss=0.053562916815280914\n",
      "Iteration 177700, loss=0.6762206554412842\n",
      "Iteration 177800, loss=0.5236888527870178\n",
      "Iteration 177900, loss=0.5051122307777405\n",
      "Iteration 178000, loss=0.2045915722846985\n",
      "Iteration 178100, loss=0.8974106311798096\n",
      "Iteration 178200, loss=0.6108043789863586\n",
      "Iteration 178300, loss=0.00127130257897079\n",
      "Iteration 178400, loss=0.04000478982925415\n",
      "Iteration 178500, loss=0.4856012165546417\n",
      "Iteration 178600, loss=1.4479788541793823\n",
      "Iteration 178700, loss=1.0069220066070557\n",
      "Iteration 178800, loss=0.4982295632362366\n",
      "Iteration 178900, loss=1.633484125137329\n",
      "Iteration 179000, loss=0.46999236941337585\n",
      "Iteration 179100, loss=0.751026451587677\n",
      "Iteration 179200, loss=0.8881810903549194\n",
      "Iteration 179300, loss=0.5005013942718506\n",
      "Iteration 179400, loss=0.5541803240776062\n",
      "Iteration 179500, loss=0.5199175477027893\n",
      "Iteration 179600, loss=0.3619180917739868\n",
      "Iteration 179700, loss=0.49406421184539795\n",
      "Iteration 179800, loss=0.4621898829936981\n",
      "Iteration 179900, loss=0.4134047031402588\n",
      "Iteration 180000, loss=0.5370519757270813\n",
      "{'november': 0.84517777, 'zero': 0.84672874, 'nine': 0.85324454, 'three': 0.8534972, 'j': 0.8538159, 'circa': 0.859352, 'm': 0.8632508, 'year': 0.86457825, 'one': 0.8874795, 'eight': 1.0}\n",
      "Iteration 180100, loss=0.11490623652935028\n",
      "Iteration 180200, loss=1.5681477785110474\n",
      "Iteration 180300, loss=0.45079442858695984\n",
      "Iteration 180400, loss=0.6566364765167236\n",
      "Iteration 180500, loss=0.5451021194458008\n",
      "Iteration 180600, loss=0.765379011631012\n",
      "Iteration 180700, loss=0.596486508846283\n",
      "Iteration 180800, loss=0.528683602809906\n",
      "Iteration 180900, loss=0.6948337554931641\n",
      "Iteration 181000, loss=0.7457484006881714\n",
      "Iteration 181100, loss=0.5114483833312988\n",
      "Iteration 181200, loss=0.6229677796363831\n",
      "Iteration 181300, loss=0.595592737197876\n",
      "Iteration 181400, loss=0.4553947448730469\n",
      "Iteration 181500, loss=0.3758217394351959\n",
      "Iteration 181600, loss=0.570583164691925\n",
      "Iteration 181700, loss=0.40808427333831787\n",
      "Iteration 181800, loss=0.5426853895187378\n",
      "Iteration 181900, loss=0.8654805421829224\n",
      "Iteration 182000, loss=0.5495439767837524\n",
      "Iteration 182100, loss=0.5322885513305664\n",
      "Iteration 182200, loss=0.005057354923337698\n",
      "Iteration 182300, loss=1.8110260963439941\n",
      "Iteration 182400, loss=0.2637963891029358\n",
      "Iteration 182500, loss=0.5476843118667603\n",
      "Iteration 182600, loss=0.9711292386054993\n",
      "Iteration 182700, loss=0.017941631376743317\n",
      "Iteration 182800, loss=0.5199589133262634\n",
      "Iteration 182900, loss=0.5487746596336365\n",
      "Iteration 183000, loss=0.5251098871231079\n",
      "Iteration 183100, loss=0.8695452809333801\n",
      "Iteration 183200, loss=0.5165599584579468\n",
      "Iteration 183300, loss=0.5469376444816589\n",
      "Iteration 183400, loss=0.9252029657363892\n",
      "Iteration 183500, loss=0.5877936482429504\n",
      "Iteration 183600, loss=0.17374365031719208\n",
      "Iteration 183700, loss=0.7245382070541382\n",
      "Iteration 183800, loss=0.5034206509590149\n",
      "Iteration 183900, loss=0.006074210628867149\n",
      "Iteration 184000, loss=0.00745010282844305\n",
      "Iteration 184100, loss=0.6791766881942749\n",
      "Iteration 184200, loss=0.545651376247406\n",
      "Iteration 184300, loss=0.7974865436553955\n",
      "Iteration 184400, loss=0.8823273181915283\n",
      "Iteration 184500, loss=0.6039286255836487\n",
      "Iteration 184600, loss=0.9453647136688232\n",
      "Iteration 184700, loss=1.0010805130004883\n",
      "Iteration 184800, loss=0.9676696062088013\n",
      "Iteration 184900, loss=0.8398030996322632\n",
      "Iteration 185000, loss=0.42929115891456604\n",
      "Iteration 185100, loss=1.1902271509170532\n",
      "Iteration 185200, loss=0.8512558937072754\n",
      "Iteration 185300, loss=0.5448101162910461\n",
      "Iteration 185400, loss=0.5452303886413574\n",
      "Iteration 185500, loss=0.9522948265075684\n",
      "Iteration 185600, loss=0.866284966468811\n",
      "Iteration 185700, loss=2.3395800590515137\n",
      "Iteration 185800, loss=0.842170238494873\n",
      "Iteration 185900, loss=0.8164829015731812\n",
      "Iteration 186000, loss=5.341704536476755e-07\n",
      "Iteration 186100, loss=0.04272567853331566\n",
      "Iteration 186200, loss=0.9091886281967163\n",
      "Iteration 186300, loss=0.013238896615803242\n",
      "Iteration 186400, loss=0.5068578124046326\n",
      "Iteration 186500, loss=0.6255632638931274\n",
      "Iteration 186600, loss=0.4914171099662781\n",
      "Iteration 186700, loss=0.4012024700641632\n",
      "Iteration 186800, loss=0.7935678362846375\n",
      "Iteration 186900, loss=0.8792188167572021\n",
      "Iteration 187000, loss=0.9260482788085938\n",
      "Iteration 187100, loss=0.720018744468689\n",
      "Iteration 187200, loss=0.5204084515571594\n",
      "Iteration 187300, loss=0.7501168251037598\n",
      "Iteration 187400, loss=0.34726810455322266\n",
      "Iteration 187500, loss=0.5931009650230408\n",
      "Iteration 187600, loss=0.5565236210823059\n",
      "Iteration 187700, loss=0.47091272473335266\n",
      "Iteration 187800, loss=0.3026561737060547\n",
      "Iteration 187900, loss=0.015843847766518593\n",
      "Iteration 188000, loss=0.7398920655250549\n",
      "Iteration 188100, loss=0.8620650768280029\n",
      "Iteration 188200, loss=0.04639773070812225\n",
      "Iteration 188300, loss=0.5290706753730774\n",
      "Iteration 188400, loss=0.34691500663757324\n",
      "Iteration 188500, loss=0.5942462086677551\n",
      "Iteration 188600, loss=0.31417223811149597\n",
      "Iteration 188700, loss=0.5335210561752319\n",
      "Iteration 188800, loss=0.9720920920372009\n",
      "Iteration 188900, loss=0.8464429974555969\n",
      "Iteration 189000, loss=0.5308797359466553\n",
      "Iteration 189100, loss=0.5415264964103699\n",
      "Iteration 189200, loss=0.4928371012210846\n",
      "Iteration 189300, loss=0.8098767995834351\n",
      "Iteration 189400, loss=0.033895038068294525\n",
      "Iteration 189500, loss=0.5183082818984985\n",
      "Iteration 189600, loss=0.3108546733856201\n",
      "Iteration 189700, loss=0.07514893263578415\n",
      "Iteration 189800, loss=0.16654212772846222\n",
      "Iteration 189900, loss=0.8161066174507141\n",
      "Iteration 190000, loss=1.2283668518066406\n",
      "{'two': 0.84240484, 'drake': 0.84309506, 'zero': 0.846987, 'nine': 0.85187024, 'circa': 0.8543262, 'year': 0.8545292, 'm': 0.8622712, 'three': 0.87067586, 'one': 0.87790954, 'eight': 1.0000001}\n",
      "Iteration 190100, loss=0.11195796728134155\n",
      "Iteration 190200, loss=0.9744599461555481\n",
      "Iteration 190300, loss=0.6577326059341431\n",
      "Iteration 190400, loss=0.5112655162811279\n",
      "Iteration 190500, loss=0.5455115437507629\n",
      "Iteration 190600, loss=0.7064142227172852\n",
      "Iteration 190700, loss=0.8632980585098267\n",
      "Iteration 190800, loss=0.9155585765838623\n",
      "Iteration 190900, loss=0.7283132076263428\n",
      "Iteration 191000, loss=0.7788527011871338\n",
      "Iteration 191100, loss=0.5661781430244446\n",
      "Iteration 191200, loss=0.5277142524719238\n",
      "Iteration 191300, loss=0.8988414406776428\n",
      "Iteration 191400, loss=0.6507508754730225\n",
      "Iteration 191500, loss=0.8867913484573364\n",
      "Iteration 191600, loss=0.4986629784107208\n",
      "Iteration 191700, loss=0.9320472478866577\n",
      "Iteration 191800, loss=0.538520336151123\n",
      "Iteration 191900, loss=1.3261305093765259\n",
      "Iteration 192000, loss=0.4052431881427765\n",
      "Iteration 192100, loss=0.5194548964500427\n",
      "Iteration 192200, loss=1.0277268886566162\n",
      "Iteration 192300, loss=0.01508281473070383\n",
      "Iteration 192400, loss=0.5205557942390442\n",
      "Iteration 192500, loss=0.5619451403617859\n",
      "Iteration 192600, loss=0.9635787010192871\n",
      "Iteration 192700, loss=0.23035822808742523\n",
      "Iteration 192800, loss=0.2618018388748169\n",
      "Iteration 192900, loss=0.5062164664268494\n",
      "Iteration 193000, loss=0.43613120913505554\n",
      "Iteration 193100, loss=0.5144481062889099\n",
      "Iteration 193200, loss=0.5451233983039856\n",
      "Iteration 193300, loss=0.7830100059509277\n",
      "Iteration 193400, loss=0.5163161754608154\n",
      "Iteration 193500, loss=0.10160863399505615\n",
      "Iteration 193600, loss=0.5011948347091675\n",
      "Iteration 193700, loss=0.4868880808353424\n",
      "Iteration 193800, loss=0.557034432888031\n",
      "Iteration 193900, loss=1.3241690397262573\n",
      "Iteration 194000, loss=0.5109381079673767\n",
      "Iteration 194100, loss=0.9684340953826904\n",
      "Iteration 194200, loss=0.5099160075187683\n",
      "Iteration 194300, loss=0.5148060917854309\n",
      "Iteration 194400, loss=0.918326735496521\n",
      "Iteration 194500, loss=0.9135887026786804\n",
      "Iteration 194600, loss=0.6673116683959961\n",
      "Iteration 194700, loss=0.5291422605514526\n",
      "Iteration 194800, loss=0.49600648880004883\n",
      "Iteration 194900, loss=0.5529650449752808\n",
      "Iteration 195000, loss=0.520865261554718\n",
      "Iteration 195100, loss=1.0450938940048218\n",
      "Iteration 195200, loss=0.650390625\n",
      "Iteration 195300, loss=0.9078773260116577\n",
      "Iteration 195400, loss=0.8493166565895081\n",
      "Iteration 195500, loss=0.009434381499886513\n",
      "Iteration 195600, loss=0.5224760174751282\n",
      "Iteration 195700, loss=0.9123120903968811\n",
      "Iteration 195800, loss=0.5354456901550293\n",
      "Iteration 195900, loss=0.5679400563240051\n",
      "Iteration 196000, loss=0.8679541349411011\n",
      "Iteration 196100, loss=0.010958398692309856\n",
      "Iteration 196200, loss=0.7109769582748413\n",
      "Iteration 196300, loss=0.8594621419906616\n",
      "Iteration 196400, loss=0.5847893357276917\n",
      "Iteration 196500, loss=0.5448948740959167\n",
      "Iteration 196600, loss=0.719352662563324\n",
      "Iteration 196700, loss=0.6221957802772522\n",
      "Iteration 196800, loss=0.0038519029039889574\n",
      "Iteration 196900, loss=0.5244441032409668\n",
      "Iteration 197000, loss=0.41254469752311707\n",
      "Iteration 197100, loss=0.9160990715026855\n",
      "Iteration 197200, loss=0.733788251876831\n",
      "Iteration 197300, loss=0.39606595039367676\n",
      "Iteration 197400, loss=0.7819589376449585\n",
      "Iteration 197500, loss=0.6872284412384033\n",
      "Iteration 197600, loss=0.030601028352975845\n",
      "Iteration 197700, loss=0.0017052004113793373\n",
      "Iteration 197800, loss=0.5095219016075134\n",
      "Iteration 197900, loss=0.35101118683815\n",
      "Iteration 198000, loss=1.0019975900650024\n",
      "Iteration 198100, loss=0.5054132342338562\n",
      "Iteration 198200, loss=0.9244054555892944\n",
      "Iteration 198300, loss=0.5292015075683594\n",
      "Iteration 198400, loss=0.45628079771995544\n",
      "Iteration 198500, loss=0.057129085063934326\n",
      "Iteration 198600, loss=0.11132099479436874\n",
      "Iteration 198700, loss=0.07458712160587311\n",
      "Iteration 198800, loss=0.9079060554504395\n",
      "Iteration 198900, loss=0.6790964007377625\n",
      "Iteration 199000, loss=0.743498682975769\n",
      "Iteration 199100, loss=0.4839840829372406\n",
      "Iteration 199200, loss=0.5314618349075317\n",
      "Iteration 199300, loss=4.227583121974021e-05\n",
      "Iteration 199400, loss=0.4411240220069885\n",
      "Iteration 199500, loss=0.29556724429130554\n",
      "Iteration 199600, loss=0.5438578724861145\n",
      "Iteration 199700, loss=0.5115063190460205\n",
      "Iteration 199800, loss=0.5956586599349976\n",
      "Iteration 199900, loss=3.9932606341608334e-06\n",
      "C:\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "C:\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1402: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n",
      "INFO:tensorflow:Assets written to: save/word2vec.100d\\assets\n"
     ]
    }
   ],
   "source": [
    "# model = keras.models.load_model(\"save/word2vec.300d\")\n",
    "log = open(\"log.\"+str(vector_dim)+\"d.txt\", \"w\")\n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(0, 200000):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 100 == 0:\n",
    "        log.write(\"Iteration {}, loss={}\\n\".format(cnt, loss))\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    if cnt % 10000 == 0:\n",
    "        # test = sim_cb.run_sim()\n",
    "        valid_sim = similar_words('eight')\n",
    "        print(valid_sim)\n",
    "        log.write(str(valid_sim) + '\\n')\n",
    "model.save('save/word2vec.'+str(vector_dim)+'d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ PRETRAINED WORD2VEC MODEL ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained():\n",
    "    embedding_dict = {}\n",
    "    path = 'pretrained/word2vec.6B/word2vec.6B.300d.txt'\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vectors = np.asarray(values[1:], 'float32')\n",
    "            embedding_dict[word] = vectors\n",
    "    f.close()\n",
    "    return embedding_dict\n",
    "\n",
    "embeddings_index = load_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_word_pretrained = \"eight\"\n",
    "sims = dict()\n",
    "emb_1 = embeddings_index[valid_word_pretrained]\n",
    "for w in embeddings_index:\n",
    "    emb_2 = embeddings_index[w]\n",
    "    sims[w] = np.dot(emb_1, emb_2) / np.linalg.norm(emb_1) / np.linalg.norm(emb_2)\n",
    "mydict = {k: v for k, v in sorted(sims.items(), key=lambda item: item[1])}\n",
    "{k: mydict[k] for k in list(mydict)[-10:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ TESTING MODELS ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = keras.models.load_model('save/word2vec.100d')\n",
    "emb_layer = model.get_layer('embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "similarity score with queen:\n0.39472947\nclosest words to king-man+woman:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'there': 0.67077494,\n",
       " 'episode': 0.6708807,\n",
       " 'aid': 0.67175865,\n",
       " 'racing': 0.6724363,\n",
       " 'and': 0.67937243,\n",
       " 'extreme': 0.6812827,\n",
       " 'woman': 0.6854999,\n",
       " 'quickly': 0.6898997,\n",
       " 'blackadder': 0.69605553,\n",
       " 'king': 0.83221656}"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# Check king-man+woman with trained model\n",
    "emb_king = np.array(emb_layer(dictionary['king']))\n",
    "emb_man = np.array(emb_layer(dictionary['man']))\n",
    "emb_woman = np.array(emb_layer(dictionary['woman']))\n",
    "emb_queen = np.array(emb_layer(dictionary['queen']))\n",
    "formula_queen = emb_king - emb_man + emb_woman\n",
    "print('similarity score with queen:')\n",
    "print(np.dot(formula_queen, emb_queen) / np.linalg.norm(formula_queen) / np.linalg.norm(emb_queen))\n",
    "print('closest words to king-man+woman:')\n",
    "sim = dict()\n",
    "emb_1 = formula_queen\n",
    "for w in dictionary:\n",
    "    emb_2 = np.array(emb_layer(dictionary[w]))\n",
    "    sim[w] = np.dot(emb_1, emb_2) / np.linalg.norm(emb_1) / np.linalg.norm(emb_2)\n",
    "mydict = {k: v for k, v in sorted(sim.items(), key=lambda item: item[1])}\n",
    "{k: mydict[k] for k in list(mydict)[-10:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "similarity score with queen:\n0.6896163\nclosest words to king-man+woman:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'elizabeth': 0.49080303,\n",
       " 'prince': 0.501774,\n",
       " 'kingdom': 0.5025345,\n",
       " 'daughter': 0.5133157,\n",
       " 'mother': 0.51421547,\n",
       " 'princess': 0.55186844,\n",
       " 'throne': 0.55653733,\n",
       " 'monarch': 0.5575491,\n",
       " 'queen': 0.6896163,\n",
       " 'king': 0.8065858}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Check king-man+woman with pre-trained model\n",
    "emb_king = np.array(embeddings_index['king'])\n",
    "emb_man = np.array(embeddings_index['man'])\n",
    "emb_woman = np.array(embeddings_index['woman'])\n",
    "emb_queen = np.array(embeddings_index['queen'])\n",
    "formula_queen = emb_king - emb_man + emb_woman\n",
    "print('similarity score with queen:')\n",
    "print(np.dot(formula_queen, emb_queen) / np.linalg.norm(formula_queen) / np.linalg.norm(emb_queen))\n",
    "print('closest words to king-man+woman:')\n",
    "sims = dict()\n",
    "emb_1 = formula_queen\n",
    "for w in embeddings_index:\n",
    "    emb_2 = embeddings_index[w]\n",
    "    sims[w] = np.dot(emb_1, emb_2) / np.linalg.norm(emb_1) / np.linalg.norm(emb_2)\n",
    "mydict = {k: v for k, v in sorted(sims.items(), key=lambda item: item[1])}\n",
    "{k: mydict[k] for k in list(mydict)[-10:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}